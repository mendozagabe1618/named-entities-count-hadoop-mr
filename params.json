{
  "name": "Named Entities Word Count",
  "tagline": "Hadoop MapReduce",
  "body": "# Named Entities Word Count\r\n\r\nDistributed for counting the named entities in from a large amount of files and counting them using a MapReduce model.\r\n\r\n### This bundle of code does a few things. ###\r\n* A python script grabs sends a HTTP GET request through the arXiv API. Then we parse the Atom XML response to get the URLs to the PDFs for the returned results. arXiv API mostly only uses PDF. It supports other formats, but for sure will support PDF. However, the implementation allows for any input type. Simply limited by the API.\r\n\r\n### Work Flow ###\r\n1. A python script, run_me.py, takes 2 command line arguments\r\n * -k \"keywords\"      - quotes included\r\n * -m maxsize\r\n2. The script then queries using the input keywords and downloads up to the maxsize of docuements.\r\n3. While this is going on. The Script silently compiles and packages the jar file.\r\n4. When ready, the script will scp\r\n1. A python script, run_me.py, prompts SBT to compile and packages all the Java source code, required binaries, etc. into a jar as well as to launch the test suite\r\n\r\n\r\n### How To Run ###\r\n* I encountered a lot of promblems building my solution into a fat jar. It was SBT that was holding me down.could not figure out how to solve the many duplicate dependencies that I had .\r\n\r\n```\r\n./script.py -h\r\nusage: script.py [-h] [-k KEYWORDS] [-m MAXRESULTS] [-s SKIPTO] [-c]\r\n\r\noptional arguments:\r\n  -h, --help            show this help message and exit\r\n  -k KEYWORDS, --keywords KEYWORDS\r\n                        keywords to searched for. in quotes if multiple\r\n  -m MAXRESULTS, --maxresults MAXRESULTS\r\n                        number of results to query = for.\r\n  -s SKIPTO, --skipto SKIPTO\r\n                        pack: skip downloading of files. must run atleast once\r\n                        hadoop: skip downloading and packaging.\r\n  -c                    Store a constant value\r\n```\r\n\r\n-c has priority, meaning that if set, the directory will be cleaning before anything else\r\n-k and -m go hand in hand. search keywords and max results    \r\ndefaults are \"computer science\" and 3, respectively\r\n-s is cool in that if you already downloaded documents with -m and -k you can, and/or packaged a jar through indirect calls to sbt package through the script, you can skip those parts so as to SKIP forward.\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}